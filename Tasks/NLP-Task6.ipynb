{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n",
    "    def __init__(self, vocab_file):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = whitespace_tokenize(text)\n",
    "        output_tokens = []\n",
    "        for token in split_tokens:\n",
    "            if token in self.vocab:\n",
    "                output_tokens.append(token)\n",
    "            else:\n",
    "                output_tokens.append(\"[UNK]\")\n",
    "        return output_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_by_vocab(self.inv_vocab, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments_from_document(document, max_segment_length):\n",
    "    \"\"\"Split single document to segments according to max_segment_length.\"\"\"\n",
    "    assert len(document) == 1\n",
    "    document = document[0]\n",
    "    document_len = len(document)\n",
    "\n",
    "    index = list(range(0, document_len, max_segment_length))\n",
    "    other_len = document_len % max_segment_length\n",
    "    if other_len > max_segment_length / 2:\n",
    "        index.append(document_len)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(index) - 1):\n",
    "        segment = document[index[i]: index[i+1]]\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "    bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "    masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "total_loss = masked_lm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"hidden_size\": 256,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"vocab_size\": 5981,\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"num_attention_heads\": 4,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"max_position_embeddings\": 256,\n",
    "  \"num_hidden_layers\": 4,\n",
    "  \"intermediate_size\": 1024,\n",
    "  \"attention_probs_dropout_prob\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
    "    # Initialise PyTorch model\n",
    "    config = BertConfig.from_json_file(bert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_output, pooled_output = \\\n",
    "    self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "if self.pooled:\n",
    "    reps = pooled_output\n",
    "else:\n",
    "    reps = sequence_output[:, 0, :]  # sen_num x 256\n",
    "\n",
    "if self.training:\n",
    "    reps = self.dropout(reps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
